Операционная система должна обеспечивать пользователю возможность предоставлять доступ к файлам и каталогам своего компьютера, публиковать папки в локальной сети. 

___

Стенд: SMB NFS FTP iSCSI Server (192.168.0.1) & SHARE Client (192.168.0.2)

Документ [[ЛКНВ.11100-01 90 03 Руководства администратора.pdf]], стр 1103-1115
# Протокол smb
## Используя консоль

Создаем папку, которая будет общей; назначаем ей права
```
mkdir /home/docstore; chmod 777 /home/docstore
```

Делаем бэкап конфигурационного файла самбы
```
mv /etc/samba/smb.conf /etc/samba/smb.conf.old
```
### Папка без пароля

Прописываем в `/etc/samba/smb.conf` следующие параметры  
```bash
[global]
dos charset = CP866
unix charset = utf8
#имя рабочей группы
workgroup = WORKGROUP
#имя сервера
server string = Filestore
#группа пользователей
security = USER
map to guest = Bad User

#имя ресурса
[Public] 
#путь к папке
path = /home/docstore 
guest ok = Yes
browseable = yes
writable = yes
create mask = 0777
directory mask = 0777
```

Создаем служебный каталог:
```bash
mkdir -p /var/lib/samba/private
```

Перезапускаем сервис SMB:
```bash
systemctl restart smb.service nmb.service
```

Включаем автозапуск:
```bash
systemctl enable smb.service nmb.service
```
### Добавление второй папки без пароля

Делаем так же как и в первом шаге, только добавляем раздел с описанием второго ресурса ниже первого.

В первом примере у нас Public и папка docstore. Во втором будет Share и files.

Создаем папку
```bash
mkdir /home/files;chmod 777 /home/files
```

Прописываем в `/etc/samba/smb.conf` следующие параметры

```bash
[Share]
path = /home/files
read only = Yes
guest ok = Yes
browseable = yes
writable = yes
create mask = 0777
directory mask = 0777
```

Перезапускаем сервис SMB:
```bash
systemctl restart smb.service nmb.service
```
### Папка с паролем

Создадим пользователя в системе, имя пользователя **share**, его пароль **1q@W3e**, при создании сделаем каталог пользователя (ключ `-m`) и зададим пароль (ключ `-p`).
```bash
useradd -m share -p 1q@W3e
```

Назначим нового владельца, пользователя share, и несколько изменим разрешения:
```bash
mkdir /home/kadry;chmod 777 /home/kadry
chown -R share:users /home/kadry
chmod -R ugo+rwx /home/kadry
```

>**Примечание:** Утилита smbpasswd находится в пакете samba-client

Добавляем пользователя в Samba (вводим пароль 1q@W3e):
```bash
smbpasswd -a share
```

Добавим в `smb.conf` следующее:
```bash
[Kadry]
comment = Кадры
path = /home/kadry
read only = no
guest ok = no
browseable = yes
writable = yes
create mask = 0777
directory mask = 0777
force user = share
force group = users
```

Папка будет доступна пользователю share с паролем 1q@W3e.

Перезапускаем сервис SMB:
```bash
systemctl restart smb.service nmb.service
```
## Используя графический интерфейс

```bash
systemctl enable --now smb
systemctl enable --now nmb
```

Создаем папку в удобной директории. Нажимаем по ней ПКМ - Опции публикации

![](/public/img/smb.png)

Настраиваем по своему желанию и нажимаем Создать публикацию.


# Протокол nfs
## Сервер NFS

### Настройка сервера NFS

#### Синтаксис `/etc/exports`

Все настройки NFS-сервера хранятся в файле `/etc/exports`.

Файл `/etc/exports` определяет, какие файловые системы экспортируются на удаленные узлы, и определяет параметры. Для этого файла действуют следующие синтаксические правила:
- пустые строки игнорируются;
- комментарии начинаются с символа решётки (#);
- для продолжения записи на новой строке можно использовать символ обратной косой черты (\);
- каждая экспортируемая ФС должна находиться на отдельной строке;
- списки авторизованных узлов, размещаемые после экспортированной файловой системы, должны быть разделены пробелами;
- параметры для каждого из узлов должны быть помещены в скобки непосредственно после идентификатора узла, без пробелов, разделяющих узел и первую скобку;
- если название экспортируемого каталога содержит пробелы, его следует заключить в двойные кавычки.

Каждая запись экспортированной файловой системы имеет следующую структуру:
```bash
export host(options)
```

Где:
- export — путь к экспортируемому каталогу;
- host — узел или сеть, в которую передается экспорт;
- options — параметры, которые будут использоваться для узла.

Можно указать несколько узлов, а также конкретные параметры для каждого узла. Для этого следует перечислить их в одной строке в виде списка, разделённого пробелами, где за каждым именем узла следуют соответствующие параметры (в скобках), например:
```bash
/mysharedir 192.168.0.100/24(no_subtree_check,rw)  192.168.10.0/24(no_subtree_check,ro)
```

Некоторые опции:
- `rw/ro` — установка разрешения доступа к ресурсу (чтение запись/только чтение);
- `no_subtree_check/subtree_check` — если экспортируется подкаталог файловой системы, но не вся файловая система, то с опцией `subtree_check` сервер, проверяет, находится ли запрошенный файл в экспортированном подкаталоге. Если экспортируется вся файловая система, запрет проверки подкаталогов (`no_subtree_check`) может увеличить скорость передачи;
- `sync/async` — синхронный/асинхронный режим доступа. Опция `sync` указывает, что сервер должен отвечать на запросы только после записи на диск изменений, выполненных этими запросами. Опция `async` указывает серверу не ждать записи информации на диск, что повышает производительность, но понижает надежность;
- `wdelay/no_wdelay` — установка задержки записи на диск (установка задержки/отключение задержки, этот параметр не действует при включенной опции async);
- `hide/nohide` — не отображать/отображать нелокальные ресурсы (например, примонтированые с помощью `mount --bind`);
- `all_squash/no_all_squash` — подмена запросов от ВСЕХ пользователей на анонимного uid/gid (либо на пользователя, заданного в параметре `anonuid/anongid`) запрет подмены uid/gid;
- `anonuid=UID` и `anongid=GID` — (явная) установка UID/GID для анонимного пользователя (может быть полезно, если какой либо каталог экспортируется для конкретного пользователя, заведенного в системе).

>**Примечание:** Единственный параметр, который следует обязательно указать — это `no_subtree_check` или ``subtree_check`.

Имя узла (узлов) может задаваться в следующей форме:
- один узел — FQDN-имя (которое сможет разрешить сервер), имя узла (которое сможет разрешить сервер) или IP-адрес;
- IP-сеть — используется запись вида a.b.c.d/z, где a.b.c.d — адрес сети, а z — префикс. Также допускается формат a.b.c.d/netmask, где a.b.c.d — сеть, а netmask — маска сети (например, 192.168.100.8/255.255.255.0);
- набор узлов — узлы определяются знаками подстановки (символ * или ?) или могут содержать списки классов символов в \[квадратных скобках\]. Знаки подстановки не должны использоваться в IP-адресах;
- сетевые группы — задаются в виде @group-name, где group-name — имя сетевой группы NIS;
- анонимный узел — задается одним символом * и будет соответствовать всем клиентам.

Для применения изменений, внесённых в файл `/etc/exports` необходимо выполнить команду:

```bash
exportfs -ra
```

или перезапустить NFS-сервер:

```bash
systemctl restart nfs
```

#### Запуск NFS под [systemd](https://www.altlinux.org/Systemd "Systemd")

Запустить NFS-сервер и включить его по умолчанию:
```bash
systemctl enable --now nfs.service 
systemctl status nfs.service
```

Если все команды прошли успешно и не выдавали ошибок, то сервер можно считать работающим. Дополнительно можно запустить команду `exportfs`, которая выведет текущие настройки на данный момент. В случае нормальной работы она должна вывести на экран записи из файла `/etc/exports`:
```bash
exportfs
/home         	192.168.0.0/24
/exports      	192.168.0.0/24
```


## Использование NFS

Подключение к NFS-серверу можно производить вручную, а можно настроить автоматическое подключение при загрузке.

### Монтирование NFS-ресурса

Команда монтирования:
```bash
mount -t nfs -o <опции> <сервер>:/remote/export /local/directory
```

Где:
- <опции> — разделённый запятыми список опций;
- <сервер> — IP-адрес или имя NFS-сервера;

`/remote/export` — каталог, экспортируемый с сервера (каталог, который будет примонтирован); /local/directory — локальный каталог (каталог, в который будет примонтирован каталог, экспортируемый с сервера).

Список доступных ресурсов можно проверить, выполнив команду:
```bash
showmount -e 192.168.0.199
```

>**Примечание:** При запуске данной команды можно получить ошибку:
```bash
showmount -e 192.168.0.199
clnt_create: RPC: Unable to receive
```

Это может происходить потому что по умолчанию `rpcbind` слушает только локальные запросы:
```bash
ss  -tnlup | grep rpcbind
# udp   UNCONN 0      0          127.0.0.1:111        0.0.0.0:*    users:(("rpcbind",pid=4248,fd=6))      
# udp   UNCONN 0      0              [::1]:111           [::]:*    users:(("rpcbind",pid=4248,fd=8))      
# tcp   LISTEN 0      4096       127.0.0.1:111        0.0.0.0:*    users:(("rpcbind",pid=4248,fd=7))      
# tcp   LISTEN 0      4096           [::1]:111           [::]:*    users:(("rpcbind",pid=4248,fd=9))
```

Разрешить `rpcbind` прослушивать входящие соединения из сети:
```bash
control rpcbind server
systemctl restart rpcbind.service
```

```bash
ss  -tnlup | grep rpcbind
# udp   UNCONN 0      0            0.0.0.0:111        0.0.0.0:*    users:(("rpcbind",pid=4130,fd=6))      
# udp   UNCONN 0      0               [::]:111           [::]:*    users:(("rpcbind",pid=4130,fd=8))      
# tcp   LISTEN 0      4096         0.0.0.0:111        0.0.0.0:*    users:(("rpcbind",pid=4130,fd=7))      
# tcp   LISTEN 0      4096            [::]:111           [::]:*    users:(("rpcbind",pid=4130,fd=9))
```
  
Пример команды монтирования (каталог /mnt/nf должен существовать:
```bash
mount -t nfs -o nfsvers=4  192.168.0.199:/home /mnt/nfs
```

Для проверки успешности монтирования можно использовать команду:
```bash
mount | grep nfs
# ...
# 192.168.0.199:/home on /mnt/nfs type nfs4 (rw,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.0.177,local_lock=none,addr=192.168.0.199)
```
Команда выдаст строку (строки) с информацией о примонтированном ресурсе (ресурсах).

### Монтирование с записью в fstab

Для автоматического монтирования к NFS-серверу при загрузке необходимо добавить следующую строку в файл `/etc/fstab`:
```bash
192.168.0.199:/home  /mnt/nfs   nfs   intr,soft,nolock,_netdev,x-systemd.automount    0 0
```
где:

- intr — позволяет прервать процесс при необходимости;
- soft — предотвращает от зависания в случае недоступности удалённой машины.

Кроме того, стоит убедиться, что сервис `netfs` запускается при старте системы.

Прежде чем изменять `/etc/fstab`, попробуйте смонтировать вручную и убедитесь, что всё работает.

### Автомонтирование

Осуществляется при помощи `automount`, `autofs` или `subfs`.

#### autofs

Данный способ монтирования позволяет автоматически монтировать папку после обращения к ней в ФМ (к примеру, через закладки) или в терминале и автоматически отмонтировать при отсутствии активности. Для реализации данного способа необходимо:

1. Установить пакет [autofs](https://packages.altlinux.org/ru/sisyphus/autofs) (если он ещё не установлен):
    ```bash
apt-get install autofs
    ```
    
2. Для настройки autofs в файл `/etc/auto.master` необходимо добавить строку:
	```bash
/mnt/nfs /etc/auto.nfs -t 60 -browse
    ```
    
Здесь `/mnt/nfs` — каталог, в котором будут подключаться сетевые файловые системы; `/etc/auto.nfs` — файл конфигурации или скрипт (определяется правами на исполнение указанного файла); `60` — таймаут (в секундах) подключения при отсутствии обращения; `browse` — создать пустой каталог для точки монтирования, чтобы предотвратить тайм-ауты, если к сетевому ресурсу невозможно подключиться.
    
3. Создать каталог `/mnt/nfs`:
    ```bash
mkdir /mnt/nfs
   ``` 
	
4. Создать файл `/etc/auto.nfs`, в который добавить строку:
    ```bash
public1  -fstype=nfs,rw 192.168.0.199:/home/
    ```
    
    где:
    - `-rw,soft,intr,rsize=8192,wsize=8192` – параметры монтирования;
    - `public1` – каталог, который будет создаваться в каталоге /mnt/nfs при монтировании;
    - `192.168.0.199:/home/`– IP-адрес и общая папка сервера.

>**Примечание:** Подключаемый ресурс должен присутствовать в выводе команды:
```bash
showmount -e 192.168.0.199
```

#### autofs + avahi

Использование avahi. Установите пакет `avahi-service-nfs` или вручную создайте на сервере файл `/etc/avahi/services/nfs.service`, используется `nfs4`:
```bash
<?xml version="1.0" standalone='no'?>
<!DOCTYPE service-group SYSTEM "avahi-service.dtd">
<service-group>
<name replace-wildcards="yes">%h</name>
<service>
       <type>_nfs._tcp</type>
       <port>2049</port>
</service>
</service-group>
```

и запустите на сервере nfs сервис `avahi-daemon`. Проверьте с машин-клиентов, что сервер nfs виден с этих машин с помощью команды:
```bash
avahi-browse -tkrp _nfs._tcp
```

Установите на машины-клиенты `autofs`. С настройками по умолчанию, ресурсы nfs будут монтироваться как `/mnt/net/servername/netshare`.

Для домашней локальной сети, когда nfs раздается не с сервера, а с другой рабочей станции, которую могут выключить в любой момент, рекомендуется прописывать у клиентов в `/etc/sysconfig/autofs timeout` поменьше (например, 5 сек):
```bash
OPTIONS="-t5"
```

#### systemd

Есть возможность использовать systemd, чтобы выполнять авто-монтирование конкретного ресурса при загрузке. Этот вариант имеет одно преимущество по сравнению с монтированием через `fstab`: если NFS-сервер по какой-либо причине будет недоступен, то клиент всё равно загрузится без ошибок.

Создаём на клиенте файл `/lib/systemd/system/nfs-shared.service` следующего содержания:
```bash
[Unit]
Description=Mount NFS share
Requires=network-online.target
After=network-online.target

[Service]
Type=oneshot
RemainAfterExit=true
ExecStart=/bin/mount 192.168.1.1:/home/data /mnt/shared -o fsc,hard
ExecStop=/bin/umount /mnt/shared
TimeoutStopSec=5

[Install]
WantedBy=multi-user.target
```

Не забываем создать каталог, в которую будет смонтирован NFS-ресурс:
```bash
mkdir -p /mnt/shared
```

После этого включаем сервис в автозагрузку и запускаем его:
```bash
systemctl enable --now nfs-shared
```


# Протокол ftp
## Настройка FTP

- Установить пакеты:
    - vsftpd
    - anonftp
- Сделать изменения в файле `/etc/xinetd.d/vsftpd`
```bash
disable = no      #включает сервис
```

- Проверить глобальные настройки `xinetd` в файле `/etc/xinetd.conf`, обратить внимание - необходимо написать либо сеть, либо адреса, у которых будет доступ к серверу:

```bash
only_from = 192.168.0.0
```

- Перезапустить сервис
```bash
systemctl enable --now xinetd
```

- Проверить, что нужное приложение (в данном случае xinetd) слушает порт:
```bash
ss -tlpn | grep "LISTEN.*:21"
```

Настройка межсетевого экрана для FTP, правила `iptables`:
```bash
$IPTABLES -A INPUT -p tcp --dport 21 -j ACCEPT
$IPTABLES -A INPUT --match state --state RELATED,ESTABLISHED -j ACCEPT
```

Облегчающий жизнь модуль ядра (разрешает RELATED): Прописывается в `/etc/net/ifaces/default/fw/iptables/modules`

```bash
modprobe ip_conntrack_ftp
```

# Протокол iSCSI
https://www.opennet.ru/base/sys/iscsi_fedora.txt.html

Для тестов используем две машины: vm01, которая будет экспортировать раздел `/dev/xvda5`, и vm02, на которой настроим инициатор.
## Настройка iSCSI Target

Для начала устанавливаем пакет `scsi-target-utils` и запускаем демон `tgtd`:
```bash
apt-get install scsi-target-utils
systemctl enable --now service tgt
```

Теперь создаем наше целевое устройство. В качестве имени я выбрал `share.server:disk1`
```bash
tgtadm --lld iscsi --op new --mode target --tid 1 -T share.server:disk1
```

В моем случае я добавляю к целевому устройству новый раздел `/dev/xvda5`:
   ```bash
tgtadm --lld iscsi --op new --mode logicalunit --tid 1 --lun 1 -b /dev/xvda5
```

Теперь разрешим доступ хосту vm02 с IP-адресом 192.168.0.109:
   ```bash
tgtadm --lld iscsi --op bind --mode target --tid 1 -I 192.168.0.109
```
   
Проверяем:
```bash
tgtadm --lld iscsi --op show --mode target
```

Под конец не забудьте прорубить "дырку" в брандмауэре для порта 3260/tcp, который используется по умолчанию.

## Настройка iSCSI Initiator

Устанавливаем пакет `iscsi-initiator-utils` и запускаем демон `iscsi`:
   ```bash
apt-get install iscsi-initiator-utils
systemctl enable --now iscsi
systemctl status iscsi
```

При запуске мы получаем совершенно справедливое сообщение, что не сконфигурирована ни одна из целей. Запускаем процесс обзора для поиска целей на хосте vm01 c IP-адресом 192.168.0.100:
```bash
iscsiadm -m discovery -t sendtargets -p 192.168.0.100:3260
```

В итоге будут созданы две поддиректории с информацией о цели и хосте:
```bash
ls /var/lib/iscsi/nodes/
# share.server:disk1
ls /var/lib/iscsi/send_targets/
# 192.168.0.100,3260
```

Просмотреть информацию можно командой:
   ```bash
iscsiadm -m node -T share.server:disk1 -p 192.168.0.100:3260
# node.name = share.server:disk1
# node.tpgt = 1
#   ...
```

Теперь, используя содержимое `/var/lib/iscsi/nodes/` и `/var/lib/iscsi/send_targets/`, демон `iscsi` при каждом перезапуске будет подключать наши ранее обнаруженные цели. 

Также процессом подключения/отключения можно управлять при помощи утилиты `iscsiadm`:
```bash
iscsiadm -m node -T share.server:disk1 -p 192.168.0.100:3260 -l
# Login session [iface: default, target: share.server:disk1, portal:  192.168.0.100,3260]
```
   
Теперь команда `fdisk` покажет наш раздел `/dev/sda`, экспортированный с vm01:
```bash
fdisk -l
# Disk /dev/xvda: 17.1 GB, 17179869184 bytes
# 255 heads, 63 sectors/track, 2088 cylinders
# Units = cylinders of 16065 * 512 = 8225280 bytes
# ...
# Disk /dev/sda: 1011 MB, 1011677184 bytes
# 32 heads, 61 sectors/track, 1012 cylinders
# Units = cylinders of 1952 * 512 = 999424 bytes
# Disk /dev/sda doesn't contain a valid partition table
```

После команды
```bash
iscsiadm -m node -T share.server:disk1 -p 192.168.0.100:3260 -u
# Logout session [sid: 2, target: share.server:disk1, portal: 192.168.0.100,3260]
``` 

в выводе `fdisk` мы увидим только `/dev/xvda`. Однако, после рестарта демона `iscsi`, цель снова появиться в списке устройств. Для удаления всей информации о цели воспользуйтесь командой:
   ```bash
iscsiadm -m node -T share.server:disk1 -p 192.168.0.100:3260 -o delete
ls /var/lib/iscsi/nodes/
```

Если у вас несколько целей, то вы не можете заранее знать, под каким именем после следующей перезагрузки/рестарта сервиса будет доступна конкретная цель. Для того, чтобы каждая цель всегда была доступна под одним и тем же именем устройства, вы можете написать соответствующее правило udev или воспользоваться монтированием по метке и UUID файловой системы. При монтировании файловых систем не забывайте использовать опцию `netdev`.